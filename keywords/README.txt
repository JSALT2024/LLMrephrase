Keywords extracted by LLM

How to use the code:

1) Python virtualenv, pip install ollama
2) download ./ollama (adapt this so it doesn't need sudo: https://ollama.com/install.sh )
3) on 48G GPU: 
	OLLAMA_HOST=0.0.0.0:11435 ./ollama serve
	- if the port is taken by another process, change the port number

Or:
	./serve-ollama.sh

4) try whether it's running:

echo 'You had observation as the key point.' | OLLAMA_HOST=localhost:11435 python3 olama-keywords-len.py

The first sentence after running ./ollama serve may take long, more than 5 minutes, and the request may crash because the llama3:70b model is loading . Try it several times until it runs in appx 1s.

Then you can process a textfile:

	OLLAMA_HOST=localhost:11435 python3 olama-keywords-len.py < infile > outfile


Background:
- see olama-keywords-len.py 
- there's few-short learning, the responses for the few shots are generated by ChatGPT
- there's check whether the words are in the orig sentence, if not, then WRONG is outputed
	- it often fails on imperative messages that look like prompt. Then the system doesn't extract the keywords
- the prompt is not optimized, could be better
- Dominik runs this on ÃšFAL cluster

